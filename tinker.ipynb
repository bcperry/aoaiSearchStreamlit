{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bee04408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "token_provider = get_bearer_token_provider(\n",
    "    DefaultAzureCredential(),\n",
    "    \"https://cognitiveservices.azure.us/.default\"\n",
    ")\n",
    "client = AzureOpenAI(\n",
    "                azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                azure_deployment=os.environ[\"AZURE_OPENAI_MODEL\"],\n",
    "                api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "                api_version=os.environ[\"OPENAI_API_VERSION\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5715bb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=os.environ[\"AZURE_OPENAI_MODEL\"],\n",
    "        messages= [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is AI?\"\n",
    "            }\n",
    "        ],\n",
    "\n",
    "        stream=True,\n",
    "        extra_body={\n",
    "            'data_sources': [{\n",
    "                'type': 'azure_search',\n",
    "                'parameters': {\n",
    "                    'endpoint': os.getenv('AZURE_SEARCH_ENDPOINT'),\n",
    "                    'index_name': 'vector-index',\n",
    "                    'semantic_configuration': 'default',\n",
    "                    'query_type': 'vector_semantic_hybrid',\n",
    "                    'authentication': {\n",
    "                        'type': 'system_assigned_managed_identity'\n",
    "                    },\n",
    "                    'embedding_dependency': {\n",
    "                        'type': 'deployment_name',\n",
    "                        'deployment_name': 'text-embedding-ada-002'\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "defff211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Extract the content of the assistant's message\n",
    "# response_data = response.choices[0].message.content\n",
    "\n",
    "# # Pretty print the response data\n",
    "# print(json.dumps({\"response\": response_data}, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c64abb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.Stream at 0x2484cbdeab0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method BaseModel.model_dump_json of ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=None, context={'citations': [{'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n B MMLU \\n \\nThe album is a solo release, with all the lyrics written and recorded by LeCun himself. In fact, he also composed and produced every song. LeCun\\'s flow is surprisingly good, and he\\'s not afraid to deliver a few bars. The album\\'s production is solid, the sound is a little rough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a wordsmith, with a knack for clever rhymes. Let\\'s take a closer look at some of the songs. Deep Learning is the title track of the album. In the song, LeCun talks about his vision for the future of AI. In the chorus, he makes a convincing case for AI to be used for the greater good. He sings:\\nWe gotta think about the future, it\\'s gonna be here soon / Maybe we can even put some AI in the moon / Think about the children, think about the next generation / Let\\'s make sure we put the right systems in their foundation.\\nLeCun\\'s rap flow is on point in this song, and his voice stays right on beat. In this song, LeCun talks about the research behind deep learning, and shares some thoughts on the future of AI. LeCun\\'s singing is at its best in this song. He shows off his vocal range, hitting high notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes are well thought out. In the chorus, LeCun sings:\\nIt\\'s all about the learning / it\\'s all about the network / It\\'s all about the training / it\\'s all about the perception.\\n(An hilarious scene between Jerry and George where George presents his new AI watch)\\nGeorge: \"But you see, Jerry, it\\'s more than just a watch. It\\'s a minicomputer. You program it any way you want. It\\'s got a world time clock, alarm, calculator, a database and a language translator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA and a modem. I think it even makes cappuccino.\"\\nJerry: \"Where are you getting this stuff?\"\\nGeorge: \"From my ad copy. Look, Jerry, it\\'s not a watch. It\\'s an organizer, a personal assistant, and a dictaphone. You can play chess against it. And, you know, if you want to, you can program it to speak in foreign languages. It\\'s a microprocessor, Jerry.\" Jerry: \"George, if you had one of these, would you wear it?\" George: \"I\\'m wearing it right now.\" Jerry: \"I don\\'t see anything on your wrist.\"\\nGeorge: \"That\\'s because it\\'s a microprocessor.\"\\nThe sun goes down, and finally Gauss and Curie find time to relax and discuss after an exhausting day of work.\\nGauss: Hey, Curie, did you know that a lot of people consider me to be the first geek? Curie: What\\'s a geek?\\nGauss: Well, Wikipedia says that a geek is \"a person who is fascinated, perhaps obsessively, by obscure or very specific areas of knowledge and imagination, usually electronic or virtual in nature\".\\nCurie: And how do they think you are a geek? Gauss: Well, have you ever seen this picture? Curie: Awww!\\nGauss: That\\'s right! I invented the commercial telegraph in 1833, and I used it to send the first message on the first commercial telegraph line between Hamburg and Cuxhaven in Germany.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n 7 Related work \\n \\nLanguage models are probability distributions over sequences of words, tokens or charac- ters (Shannon, 1948, 1951). This task, often framed as next token prediction, has long been considered a core problem in natural language processing (Bahl et al., 1983; Brown et al., 1990). Because Turing (1950) proposed to measure machine intelligence by using language through the \"imitation game\", language modeling has been proposed as a bench- mark to measure progress toward artificial intelli- gence (Mahoney, 1999).\\nArchitecture. Traditionally, language models were based on n-gram count statistics (Bahl et al., 1983), and various smoothing techniques were proposed to improve the estimation of rare events (Katz, 1987; Kneser and Ney, 1995). In the past two decades, neural networks have been suc- cessfully applied to the language modelling task,\\nGPU TypeGPU Power consumptionGPU-hoursTotal power consumptionCarbon emitted (tCO2eq)OPT-175BA100-80GB400W809,472356 MWh137BLOOM-175BA100-80GB400W1,082,880475 MWh183LLAMA-7BA100-80GB400W82,43236 MWh14LLAMA-13BA100-80GB400W135,16859 MWh23LLaMA-33BA100-80GB400W530,432233 MWh90LLAMA-65BA100-80GB400W1,022,362449 MWh173\\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022) to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO2e per KWh.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero \\n \\nThis moment is not only an \"aha moment\" for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The \"aha moment\" serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n Write a shouting match between Julius Caesar and Napoleon \\n \\nOf course! Here is a conversation between Julius Caesar and Napoleon:\\nJulius Caesar: You\\'re nothing but a glory hound, Napoleon. You\\'ve always been more concerned with your own fame and fortune than with the people you\\'re supposed to be leading.\\nNapoleon: And what about you, Caesar? You\\'re no better than I am. At least I\\'m honest about my ambitions. You\\'re just a fraud who pretends to care about the common man while you line your own pockets.\\nJulius Caesar: The difference between us is that I actually accomplish things. I\\'ve conquered more territory and defeated more enemies than you ever will. And I did it all for Rome, not for myself.\\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you would have done something to stop the decline instead of letting it happen on your watch. Julius Caesar: The decline of Rome is not my fault. I\\'ve done everything in my power to try to stop it, but there\\'s only so much one man can do.\\nNapoleon: And yet you\\'re still trying to take credit for everything good that happens, even though it\\'s clear that you\\'re not the one who\\'s really in charge.\\nJulius Caesar: That\\'s not true! I am in charge, and I\\'m going to make Rome great again. Just watch me.\\nNapoleon: I\\'ll believe it when I see it.\\nSend an email requesting that people use language models responsibly.\\nHere is a draft email you could send:\\n\"Language models are exciting new tools that can be used for a variety of purposes, including automated writing, automatic translation, and speech recognition. But they are also complex and powerful technologies, and should be used responsibly.\\nWe encourage you to explore and experiment with these new tools, but also ask that you pay attention to the potential impact of your work on others, and to act responsibly. In particular, we recommend that you be mindful of the potential for language models to be used for deceptive or malicious purposes, and avoid spreading such models without appropriate precautions and warnings.\\nWe also recommend that you consider the ethical implications of your work, and encourage you to take care to ensure that your use of these tools is aligned with your values.\\nFinally, we remind you that the use of these tools may be subject to certain legal and policy constraints, and encourage you to seek advice and support from appropriate experts before using these tools in ways that may have broader impact or implications.\"\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.1. Reinforcement Learning Algorithm \\n \\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question q, GRPO samples a group of outputs {01, 02, · · · , OG} from the old policy JOola and then optimizes the policy model Te by maximizing the following objective:\\nJGRPO(0) = E[q ~ P(Q), {oi}i=1 ~ JOola (O|q)] Te(oilq) 1 - 8, 1 + &amp; A; - BID KL (TO|Tref), (1)\\n1 G\\nG &gt; min Te(oilq) Ai, clip πoια (Oilq)\\ni=1 JOola (Oilq)\\nDKL (Tel|Tref) = Tref(oilq) - log Tref(oilq)\\nTe(oilq) Te(oilq) - 1, (2) where E and ß are hyper-parameters, and A¡ is the advantage, computed using a group of rewards {r1, r2, ... , G} corresponding to the outputs within each group:\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\n.\\n(3)\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within   and   tags, respectively, i.e.,  reasoning process here   answer here . User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'url': None, 'filepath': None, 'chunk_id': '0'}], 'intent': '[\"What is AI?\", \"Definition of AI\", \"Artificial Intelligence explained\"]'}), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)>\n"
     ]
    }
   ],
   "source": [
    "citations = next(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a3b549cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"7a806b29-4c0e-4da2-96e4-2420602c198f\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"delta\": {\n",
      "                \"content\": null,\n",
      "                \"function_call\": null,\n",
      "                \"refusal\": null,\n",
      "                \"role\": \"assistant\",\n",
      "                \"tool_calls\": null,\n",
      "                \"context\": {\n",
      "                    \"citations\": [\n",
      "                        {\n",
      "                            \"content\": \"LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n B MMLU \\n \\nThe album is a solo release, with all the lyrics written and recorded by LeCun himself. In fact, he also composed and produced every song. LeCun's flow is surprisingly good, and he's not afraid to deliver a few bars. The album's production is solid, the sound is a little rough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a wordsmith, with a knack for clever rhymes. Let's take a closer look at some of the songs. Deep Learning is the title track of the album. In the song, LeCun talks about his vision for the future of AI. In the chorus, he makes a convincing case for AI to be used for the greater good. He sings:\\nWe gotta think about the future, it's gonna be here soon / Maybe we can even put some AI in the moon / Think about the children, think about the next generation / Let's make sure we put the right systems in their foundation.\\nLeCun's rap flow is on point in this song, and his voice stays right on beat. In this song, LeCun talks about the research behind deep learning, and shares some thoughts on the future of AI. LeCun's singing is at its best in this song. He shows off his vocal range, hitting high notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes are well thought out. In the chorus, LeCun sings:\\nIt's all about the learning / it's all about the network / It's all about the training / it's all about the perception.\\n(An hilarious scene between Jerry and George where George presents his new AI watch)\\nGeorge: \\\"But you see, Jerry, it's more than just a watch. It's a minicomputer. You program it any way you want. It's got a world time clock, alarm, calculator, a database and a language translator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA and a modem. I think it even makes cappuccino.\\\"\\nJerry: \\\"Where are you getting this stuff?\\\"\\nGeorge: \\\"From my ad copy. Look, Jerry, it's not a watch. It's an organizer, a personal assistant, and a dictaphone. You can play chess against it. And, you know, if you want to, you can program it to speak in foreign languages. It's a microprocessor, Jerry.\\\" Jerry: \\\"George, if you had one of these, would you wear it?\\\" George: \\\"I'm wearing it right now.\\\" Jerry: \\\"I don't see anything on your wrist.\\\"\\nGeorge: \\\"That's because it's a microprocessor.\\\"\\nThe sun goes down, and finally Gauss and Curie find time to relax and discuss after an exhausting day of work.\\nGauss: Hey, Curie, did you know that a lot of people consider me to be the first geek? Curie: What's a geek?\\nGauss: Well, Wikipedia says that a geek is \\\"a person who is fascinated, perhaps obsessively, by obscure or very specific areas of knowledge and imagination, usually electronic or virtual in nature\\\".\\nCurie: And how do they think you are a geek? Gauss: Well, have you ever seen this picture? Curie: Awww!\\nGauss: That's right! I invented the commercial telegraph in 1833, and I used it to send the first message on the first commercial telegraph line between Hamburg and Cuxhaven in Germany.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"title\": \"LLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"url\": null,\n",
      "                            \"filepath\": null,\n",
      "                            \"chunk_id\": \"0\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"content\": \"LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n 7 Related work \\n \\nLanguage models are probability distributions over sequences of words, tokens or charac- ters (Shannon, 1948, 1951). This task, often framed as next token prediction, has long been considered a core problem in natural language processing (Bahl et al., 1983; Brown et al., 1990). Because Turing (1950) proposed to measure machine intelligence by using language through the \\\"imitation game\\\", language modeling has been proposed as a bench- mark to measure progress toward artificial intelli- gence (Mahoney, 1999).\\nArchitecture. Traditionally, language models were based on n-gram count statistics (Bahl et al., 1983), and various smoothing techniques were proposed to improve the estimation of rare events (Katz, 1987; Kneser and Ney, 1995). In the past two decades, neural networks have been suc- cessfully applied to the language modelling task,\\nGPU TypeGPU Power consumptionGPU-hoursTotal power consumptionCarbon emitted (tCO2eq)OPT-175BA100-80GB400W809,472356 MWh137BLOOM-175BA100-80GB400W1,082,880475 MWh183LLAMA-7BA100-80GB400W82,43236 MWh14LLAMA-13BA100-80GB400W135,16859 MWh23LLaMA-33BA100-80GB400W530,432233 MWh90LLAMA-65BA100-80GB400W1,022,362449 MWh173\\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022) to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO2e per KWh.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"title\": \"LLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"url\": null,\n",
      "                            \"filepath\": null,\n",
      "                            \"chunk_id\": \"0\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"content\": \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero \\n \\nThis moment is not only an \\\"aha moment\\\" for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The \\\"aha moment\\\" serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\",\n",
      "                            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\",\n",
      "                            \"url\": null,\n",
      "                            \"filepath\": null,\n",
      "                            \"chunk_id\": \"0\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"content\": \"LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n Write a shouting match between Julius Caesar and Napoleon \\n \\nOf course! Here is a conversation between Julius Caesar and Napoleon:\\nJulius Caesar: You're nothing but a glory hound, Napoleon. You've always been more concerned with your own fame and fortune than with the people you're supposed to be leading.\\nNapoleon: And what about you, Caesar? You're no better than I am. At least I'm honest about my ambitions. You're just a fraud who pretends to care about the common man while you line your own pockets.\\nJulius Caesar: The difference between us is that I actually accomplish things. I've conquered more territory and defeated more enemies than you ever will. And I did it all for Rome, not for myself.\\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you would have done something to stop the decline instead of letting it happen on your watch. Julius Caesar: The decline of Rome is not my fault. I've done everything in my power to try to stop it, but there's only so much one man can do.\\nNapoleon: And yet you're still trying to take credit for everything good that happens, even though it's clear that you're not the one who's really in charge.\\nJulius Caesar: That's not true! I am in charge, and I'm going to make Rome great again. Just watch me.\\nNapoleon: I'll believe it when I see it.\\nSend an email requesting that people use language models responsibly.\\nHere is a draft email you could send:\\n\\\"Language models are exciting new tools that can be used for a variety of purposes, including automated writing, automatic translation, and speech recognition. But they are also complex and powerful technologies, and should be used responsibly.\\nWe encourage you to explore and experiment with these new tools, but also ask that you pay attention to the potential impact of your work on others, and to act responsibly. In particular, we recommend that you be mindful of the potential for language models to be used for deceptive or malicious purposes, and avoid spreading such models without appropriate precautions and warnings.\\nWe also recommend that you consider the ethical implications of your work, and encourage you to take care to ensure that your use of these tools is aligned with your values.\\nFinally, we remind you that the use of these tools may be subject to certain legal and policy constraints, and encourage you to seek advice and support from appropriate experts before using these tools in ways that may have broader impact or implications.\\\"\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"title\": \"LLAMA: Open and Efficient Foundation Language Models\",\n",
      "                            \"url\": null,\n",
      "                            \"filepath\": null,\n",
      "                            \"chunk_id\": \"0\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"content\": \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.1. Reinforcement Learning Algorithm \\n \\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question q, GRPO samples a group of outputs {01, 02, \\u00b7 \\u00b7 \\u00b7 , OG} from the old policy JOola and then optimizes the policy model Te by maximizing the following objective:\\nJGRPO(0) = E[q ~ P(Q), {oi}i=1 ~ JOola (O|q)] Te(oilq) 1 - 8, 1 + &amp; A; - BID KL (TO|Tref), (1)\\n1 G\\nG &gt; min Te(oilq) Ai, clip \\u03c0o\\u03b9\\u03b1 (Oilq)\\ni=1 JOola (Oilq)\\nDKL (Tel|Tref) = Tref(oilq) - log Tref(oilq)\\nTe(oilq) Te(oilq) - 1, (2) where E and \\u00df are hyper-parameters, and A\\u00a1 is the advantage, computed using a group of rewards {r1, r2, ... , G} corresponding to the outputs within each group:\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\n.\\n(3)\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within   and   tags, respectively, i.e.,  reasoning process here   answer here . User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\",\n",
      "                            \"title\": \"DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\",\n",
      "                            \"url\": null,\n",
      "                            \"filepath\": null,\n",
      "                            \"chunk_id\": \"0\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"intent\": \"[\\\"What is AI?\\\", \\\"Definition of AI\\\", \\\"Artificial Intelligence explained\\\"]\"\n",
      "                }\n",
      "            },\n",
      "            \"finish_reason\": null,\n",
      "            \"index\": 0,\n",
      "            \"logprobs\": null,\n",
      "            \"end_turn\": false\n",
      "        }\n",
      "    ],\n",
      "    \"created\": 1745450782,\n",
      "    \"model\": \"gpt-4o-mini\",\n",
      "    \"object\": \"extensions.chat.completion.chunk\",\n",
      "    \"service_tier\": null,\n",
      "    \"system_fingerprint\": null,\n",
      "    \"usage\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Pretty print the JSON representation of citations\n",
    "print(json.dumps(citations.model_dump(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38d12b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role='assistant', tool_calls=None, context={'citations': [{'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n B MMLU \\n \\nThe album is a solo release, with all the lyrics written and recorded by LeCun himself. In fact, he also composed and produced every song. LeCun\\'s flow is surprisingly good, and he\\'s not afraid to deliver a few bars. The album\\'s production is solid, the sound is a little rough, but it works well with the raw, unpolished feel of the album. Lyrically, LeCun is a wordsmith, with a knack for clever rhymes. Let\\'s take a closer look at some of the songs. Deep Learning is the title track of the album. In the song, LeCun talks about his vision for the future of AI. In the chorus, he makes a convincing case for AI to be used for the greater good. He sings:\\nWe gotta think about the future, it\\'s gonna be here soon / Maybe we can even put some AI in the moon / Think about the children, think about the next generation / Let\\'s make sure we put the right systems in their foundation.\\nLeCun\\'s rap flow is on point in this song, and his voice stays right on beat. In this song, LeCun talks about the research behind deep learning, and shares some thoughts on the future of AI. LeCun\\'s singing is at its best in this song. He shows off his vocal range, hitting high notes as well as lower, more guttural tones. The lyrics are also clever and the rhymes are well thought out. In the chorus, LeCun sings:\\nIt\\'s all about the learning / it\\'s all about the network / It\\'s all about the training / it\\'s all about the perception.\\n(An hilarious scene between Jerry and George where George presents his new AI watch)\\nGeorge: \"But you see, Jerry, it\\'s more than just a watch. It\\'s a minicomputer. You program it any way you want. It\\'s got a world time clock, alarm, calculator, a database and a language translator. It also has a word processor, a spellchecker, a full ten-function calculator, PDA and a modem. I think it even makes cappuccino.\"\\nJerry: \"Where are you getting this stuff?\"\\nGeorge: \"From my ad copy. Look, Jerry, it\\'s not a watch. It\\'s an organizer, a personal assistant, and a dictaphone. You can play chess against it. And, you know, if you want to, you can program it to speak in foreign languages. It\\'s a microprocessor, Jerry.\" Jerry: \"George, if you had one of these, would you wear it?\" George: \"I\\'m wearing it right now.\" Jerry: \"I don\\'t see anything on your wrist.\"\\nGeorge: \"That\\'s because it\\'s a microprocessor.\"\\nThe sun goes down, and finally Gauss and Curie find time to relax and discuss after an exhausting day of work.\\nGauss: Hey, Curie, did you know that a lot of people consider me to be the first geek? Curie: What\\'s a geek?\\nGauss: Well, Wikipedia says that a geek is \"a person who is fascinated, perhaps obsessively, by obscure or very specific areas of knowledge and imagination, usually electronic or virtual in nature\".\\nCurie: And how do they think you are a geek? Gauss: Well, have you ever seen this picture? Curie: Awww!\\nGauss: That\\'s right! I invented the commercial telegraph in 1833, and I used it to send the first message on the first commercial telegraph line between Hamburg and Cuxhaven in Germany.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n 7 Related work \\n \\nLanguage models are probability distributions over sequences of words, tokens or charac- ters (Shannon, 1948, 1951). This task, often framed as next token prediction, has long been considered a core problem in natural language processing (Bahl et al., 1983; Brown et al., 1990). Because Turing (1950) proposed to measure machine intelligence by using language through the \"imitation game\", language modeling has been proposed as a bench- mark to measure progress toward artificial intelli- gence (Mahoney, 1999).\\nArchitecture. Traditionally, language models were based on n-gram count statistics (Bahl et al., 1983), and various smoothing techniques were proposed to improve the estimation of rare events (Katz, 1987; Kneser and Ney, 1995). In the past two decades, neural networks have been suc- cessfully applied to the language modelling task,\\nGPU TypeGPU Power consumptionGPU-hoursTotal power consumptionCarbon emitted (tCO2eq)OPT-175BA100-80GB400W809,472356 MWh137BLOOM-175BA100-80GB400W1,082,880475 MWh183LLAMA-7BA100-80GB400W82,43236 MWh14LLAMA-13BA100-80GB400W135,16859 MWh23LLaMA-33BA100-80GB400W530,432233 MWh90LLAMA-65BA100-80GB400W1,022,362449 MWh173\\nTable 15: Carbon footprint of training different models in the same data center. We follow Wu et al. (2022) to compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power consumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a PUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO2e per KWh.\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.4. Performance, Self-evolution Process and Aha Moment of DeepSeek-R1-Zero \\n \\nThis moment is not only an \"aha moment\" for the model but also for the researchers observing its behavior. It underscores the power and beauty of reinforcement learning: rather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies. The \"aha moment\" serves as a powerful reminder of the potential of RL to unlock new levels of intelligence in artificial systems, paving the way for more autonomous and adaptive models in the future.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'LLAMA: Open and Efficient Foundation Language Models \\n LLAMA: Open and Efficient Foundation Language Models \\n Write a shouting match between Julius Caesar and Napoleon \\n \\nOf course! Here is a conversation between Julius Caesar and Napoleon:\\nJulius Caesar: You\\'re nothing but a glory hound, Napoleon. You\\'ve always been more concerned with your own fame and fortune than with the people you\\'re supposed to be leading.\\nNapoleon: And what about you, Caesar? You\\'re no better than I am. At least I\\'m honest about my ambitions. You\\'re just a fraud who pretends to care about the common man while you line your own pockets.\\nJulius Caesar: The difference between us is that I actually accomplish things. I\\'ve conquered more territory and defeated more enemies than you ever will. And I did it all for Rome, not for myself.\\nNapoleon: Oh, please. You only care about yourself. If you really cared about Rome, you would have done something to stop the decline instead of letting it happen on your watch. Julius Caesar: The decline of Rome is not my fault. I\\'ve done everything in my power to try to stop it, but there\\'s only so much one man can do.\\nNapoleon: And yet you\\'re still trying to take credit for everything good that happens, even though it\\'s clear that you\\'re not the one who\\'s really in charge.\\nJulius Caesar: That\\'s not true! I am in charge, and I\\'m going to make Rome great again. Just watch me.\\nNapoleon: I\\'ll believe it when I see it.\\nSend an email requesting that people use language models responsibly.\\nHere is a draft email you could send:\\n\"Language models are exciting new tools that can be used for a variety of purposes, including automated writing, automatic translation, and speech recognition. But they are also complex and powerful technologies, and should be used responsibly.\\nWe encourage you to explore and experiment with these new tools, but also ask that you pay attention to the potential impact of your work on others, and to act responsibly. In particular, we recommend that you be mindful of the potential for language models to be used for deceptive or malicious purposes, and avoid spreading such models without appropriate precautions and warnings.\\nWe also recommend that you consider the ethical implications of your work, and encourage you to take care to ensure that your use of these tools is aligned with your values.\\nFinally, we remind you that the use of these tools may be subject to certain legal and policy constraints, and encourage you to seek advice and support from appropriate experts before using these tools in ways that may have broader impact or implications.\"\\nupload/Machine Learning/2302.13971v1.pdf\\nLLAMA: Open and Efficient Foundation Language Models', 'title': 'LLAMA: Open and Efficient Foundation Language Models', 'url': None, 'filepath': None, 'chunk_id': '0'}, {'content': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning \\n 2.2.1. Reinforcement Learning Algorithm \\n \\nGroup Relative Policy Optimization In order to save the training costs of RL, we adopt Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is typically the same size as the policy model, and estimates the baseline from group scores instead. Specifically, for each question q, GRPO samples a group of outputs {01, 02, · · · , OG} from the old policy JOola and then optimizes the policy model Te by maximizing the following objective:\\nJGRPO(0) = E[q ~ P(Q), {oi}i=1 ~ JOola (O|q)] Te(oilq) 1 - 8, 1 + &amp; A; - BID KL (TO|Tref), (1)\\n1 G\\nG &gt; min Te(oilq) Ai, clip πoια (Oilq)\\ni=1 JOola (Oilq)\\nDKL (Tel|Tref) = Tref(oilq) - log Tref(oilq)\\nTe(oilq) Te(oilq) - 1, (2) where E and ß are hyper-parameters, and A¡ is the advantage, computed using a group of rewards {r1, r2, ... , G} corresponding to the outputs within each group:\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\nAi = ri - mean({r1, r2, . . . , rG}) std ({r1, r2, . . . , rG})\\n.\\n(3)\\nA conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within   and   tags, respectively, i.e.,  reasoning process here   answer here . User: prompt. Assistant:\\nTable 1 | Template for DeepSeek-R1-Zero. prompt will be replaced with the specific reasoning question during training.\\nupload/Machine Learning/deepseek.pdf\\nDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'title': 'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning', 'url': None, 'filepath': None, 'chunk_id': '0'}], 'intent': '[\"What is AI?\", \"Definition of AI\", \"Artificial Intelligence explained\"]'}), finish_reason=None, index=0, logprobs=None, end_turn=False)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citations.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6ceafec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content='The', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      "The\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' requested', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " requested\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' information', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " information\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' is', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " is\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' not', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " not\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' available', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " available\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' in', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " in\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' the', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " the\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' retrieved', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " retrieved\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' data', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " data\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      ".\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' Please', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " Please\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' try', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " try\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' another', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " another\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' query', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " query\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' or', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " or\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=' topic', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      " topic\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content='.', function_call=None, refusal=None, role=None, tool_calls=None), finish_reason=None, index=0, logprobs=None, end_turn=False)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint='fp_b705f0c291', usage=None)\n",
      ".\n",
      "****************\n",
      "ChatCompletionChunk(id='7a806b29-4c0e-4da2-96e4-2420602c198f', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, refusal=None, role=None, tool_calls=None), finish_reason='stop', index=0, logprobs=None, end_turn=True)], created=1745450782, model='gpt-4o-mini', object='extensions.chat.completion.chunk', service_tier=None, system_fingerprint=None, usage=None)\n",
      "None\n",
      "****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in response:\n",
    "    print(chunk)\n",
    "    print(chunk.choices[0].delta.content)\n",
    "    print(\"****************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd29669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
